from __future__ import annotations

"""
Streamlit Humanizer (stability & realism pass)
- –ù–∞–¥—ë–∂–Ω–∞—è JSON-–æ–±—Ä–∞–±–æ—Ç–∫–∞ (response_format="json_object")
- –ë–∞—Ç—á–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ HTML (–ø–æ —Å–∏–º–≤–æ–ª–∞–º)
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, top_p, —á–∞—Å—Ç–æ—Ç–Ω—ã–µ —à—Ç—Ä–∞—Ñ—ã)
- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä [Words: N]
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –º–µ–ª–∫–∏–µ –±–∞–≥–∏ (–≤ —Ç.—á. —Å–ª—É—á–∞–π–Ω—ã–π —Å–∏–º–≤–æ–ª '—ë')
- –î–æ–ø. –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è HTML-—É–∑–ª–æ–≤ (—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —Ç–µ–≥)

‚ö†Ô∏è –≠—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –∞ –Ω–µ –¥–ª—è –Ω–∞—Ä—É—à–µ–Ω–∏—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —á–µ—Å—Ç–Ω–æ—Å—Ç–∏.
"""

import io
import json
import math
import re
import tempfile
from dataclasses import dataclass
from typing import Dict, Iterable, Iterator, List, Tuple

import streamlit as st
from bs4 import BeautifulSoup, NavigableString, Tag
from openai import OpenAI

# --- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ—Ñ–∏—Å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ---
try:
    from docx import Document  # .docx
except Exception:
    Document = None

try:
    import textract  # .doc (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –µ—Å—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
except Exception:
    textract = None

import os

# ----------------------------
# –ö–ª—é—á –∏ –º–æ–¥–µ–ª—å –∏–∑ Streamlit Secrets / –æ–∫—Ä—É–∂–µ–Ω–∏—è
# ----------------------------
API_KEY_DEFAULT = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY", "")
MODEL_DEFAULT   = st.secrets.get("OPENAI_MODEL") or os.getenv("OPENAI_MODEL", "gpt-5")

# ----------------------------
# –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
# ----------------------------
st.set_page_config(page_title="Humanizer ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è", page_icon="üõ†Ô∏è", layout="wide")
st.title("üõ†Ô∏è Humanizer —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è")

st.caption(
    "–ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å: JSON-mode, –±–∞—Ç—á–∏–Ω–≥ –¥–ª–∏–Ω–Ω—ã—Ö HTML, –∫–æ–Ω—Ç—Ä–æ–ª—å –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–≥–æ–≤."
)

# ----------------------------
# –•–µ–ª–ø–µ—Ä—ã
# ----------------------------

def is_html(text: str) -> bool:
    if not text:
        return False
    # –ë—ã—Å—Ç—Ä—ã–π —Ä–∞–Ω–Ω–∏–π —Ç–µ—Å—Ç –ø–æ —É–≥–ª–æ–≤—ã–º —Å–∫–æ–±–∫–∞–º
    if "</" in text or "/>" in text or re.search(r"<([a-zA-Z][^>]*?)>", text):
        return True
    # –§–æ–ª–±—ç–∫ —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–µ—Ä
    soup = BeautifulSoup(text, "lxml")
    return bool(soup.find())


def _word_count(s: str) -> int:
    tokens = re.findall(r"\w+", s, flags=re.UNICODE)
    return len(tokens)


def append_words_marker_to_html(html: str, n: int) -> str:
    try:
        soup = BeautifulSoup(html, "lxml")
        container = soup.body or soup
        p = soup.new_tag("p")
        p.string = f"[Words: {n}]"
        container.append(p)
        return str(soup)
    except Exception:
        return f"{html}\n[Words: {n}]"


# ------------ HTML —Ä–∞–∑–º–µ—Ç–∫–∞ ‚Üí –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤ -------------
@dataclass
class NodeInfo:
    text: str
    parent_tag: str


def extract_text_nodes_as_mapping(html: str) -> Tuple[str, Dict[str, NodeInfo]]:
    """–û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã –≤ <span data-hid="..."> –∏ —Å–æ–±–∏—Ä–∞–µ—Ç mapping id->NodeInfo."""
    soup = BeautifulSoup(html, "lxml")

    for bad in soup(["script", "style", "noscript"]):
        bad.extract()

    hid_counter = 0
    mapping: Dict[str, NodeInfo] = {}

    def tag_text_nodes(t: Tag) -> None:
        nonlocal hid_counter
        for child in list(t.children):
            if isinstance(child, NavigableString):
                text = str(child)
                if text and text.strip():
                    hid_counter += 1
                    hid = f"t{hid_counter}"
                    span = soup.new_tag("span")
                    span["data-hid"] = hid
                    span.string = text
                    child.replace_with(span)
                    parent_tag = t.name.lower() if isinstance(t, Tag) and t.name else "div"
                    mapping[hid] = NodeInfo(text=text, parent_tag=parent_tag)
            elif isinstance(child, Tag):
                tag_text_nodes(child)

    tag_text_nodes(soup.body or soup)
    return str(soup), mapping


def replace_text_nodes_from_mapping(html_with_ids: str, replacements: Dict[str, str]) -> str:
    soup = BeautifulSoup(html_with_ids, "lxml")
    for span in soup.find_all(attrs={"data-hid": True}):
        hid = span.get("data-hid")
        if hid in replacements:
            span.string = replacements[hid]
    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
    for span in soup.find_all(attrs={"data-hid": True}):
        del span["data-hid"]
    return str(soup)


# ----------------------------
# –ü—Ä–æ–º–ø—Ç—ã (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
# ----------------------------
PROMPT_PLAIN_TEXT_TPL = """You are an expert editor.
Edit the text so it reads naturally for a native speaker while preserving the original meaning, structure, and tone.

Language: use the SAME language as the input (auto-detect). Do NOT translate.

Requirements:
- Word count: keep within ¬±{wc_tol}% of the original. Append the final count as [Words: N].
- Structure: keep paragraph breaks, headings, list order/numbering, and overall section order.
- Formatting: keep punctuation, quotation marks, inline formatting (bold/italic/links/code), emojis, citation markers, and references.
- Facts & entities: do not add, remove, or alter information. Keep names, numbers, dates, URLs, and titles unchanged.
- Tone & register: preserve the author‚Äôs voice and level of formality.
- Style tweaks: use idiomatic phrasing, reduce repetitiveness, vary sentence length, simplify clunky constructions‚Äîwithout changing emphasis or intent.
- Non-text elements (code, formulas, tables): leave unchanged.
- Return ONLY the edited text‚Äîno explanations, no metadata (besides [Words: N]).
"""

PROMPT_PLAIN_TO_HTML_TPL = """You are an expert editor and HTML formatter.
Edit the text so it reads naturally (same language) and then output clean, semantic HTML.

Requirements:
- Word count: keep within ¬±{wc_tol}% of the original. Append the final count as a visible last paragraph: [Words: N].
- Preserve paragraph breaks, headings, list order/numbering, and section order (convert to HTML).
- Preserve punctuation, quotes, inline emphasis/links/code semantics; convert markers to <strong>/<em>/<a>/<code>.
- Do not alter facts, names, numbers, dates, URLs, or titles.
- Keep the author‚Äôs voice; improve fluency without changing intent.
- If the edited content contains at least one <table>, include at the VERY TOP a single <style> block:
  <style>
  table { border-collapse: collapse; }
  table, th, td { border: 1px solid #000; }
  </style>
- Non-text elements (code blocks, formulas, tables) should be kept as-is, wrapped in proper HTML tags if present.
- Return ONLY the HTML markup. No markdown, no comments, no code fences.
- Allowed tags: style (single block as above), p, h1..h4, ul/ol/li, blockquote, pre, code, a, strong, em, table/thead/tbody/tr/th/td, img (only if present), span.
"""

PROMPT_HTML_JSON_TPL = """You are an expert micro-editor.
You will receive a JSON object mapping {{id: text}} extracted from HTML text nodes.
For each value: edit to read naturally while preserving meaning, structure, tone, punctuation and references.
Use the SAME language as the value. Do NOT translate.

Constraints PER VALUE:
- Word count: keep within ¬±{wc_tol}% of that value‚Äôs original length.
- Absolutely DO NOT introduce or remove HTML tags; you are editing TEXT CONTENT ONLY.
- Return ONLY a valid JSON object with the SAME KEYS and improved string values. No extra text.

Reference (do not include in JSON): here are parent tag names for each id to help with style.
{{parents}}
"""

# ----------------------------
# –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—å—é
# ----------------------------

def _openai_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)


def _call_openai_json_map(
    client: OpenAI,
    model: str,
    mapping: Dict[str, NodeInfo],
    wc_tol: int,
    temperature: float,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
    seed: int | None = None,
) -> Dict[str, str]:
    """–í—ã–∑—ã–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ JSON-—Ä–µ–∂–∏–º–µ –±–∞—Ç—á–∞–º–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π dict id->text."""
    # –ì–æ—Ç–æ–≤–∏–º –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    id_to_text = {k: v.text for k, v in mapping.items()}
    id_to_parent = {k: v.parent_tag for k, v in mapping.items()}

    # –ë–∞—Ç—á–∏–Ω–≥ –ø–æ —Å—É–º–º–∞—Ä–Ω–æ–π –¥–ª–∏–Ω–µ –∑–Ω–∞—á–µ–Ω–∏–π (—Å–∏–º–≤–æ–ª—ã), —á—Ç–æ–±—ã –Ω–µ —É–ø–∏—Ä–∞—Ç—å—Å—è –≤ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
    batches: List[List[str]] = []
    current: List[str] = []
    acc_len = 0
    MAX_CHARS = 12000  # —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π

    for k, v in id_to_text.items():
        add_len = len(k) + len(v) + 6
        if acc_len + add_len > MAX_CHARS and current:
            batches.append(current)
            current = []
            acc_len = 0
        current.append(k)
        acc_len += add_len
    if current:
        batches.append(current)

    out: Dict[str, str] = {}

    for i, batch_keys in enumerate(batches, 1):
        sub_map = {k: id_to_text[k] for k in batch_keys}
        parents_hint = {k: id_to_parent[k] for k in batch_keys}

        prompt = PROMPT_HTML_JSON_TPL.format(wc_tol=wc_tol, parents=json.dumps(parents_hint, ensure_ascii=False))

        # JSON-mode –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä–æ–≥–æ JSON
        kwargs = dict(
            model=model,
            messages=[
                {"role": "system", "content": "You are a careful, detail-oriented text editor."},
                {"role": "user", "content": prompt},
                {"role": "user", "content": json.dumps(sub_map, ensure_ascii=False)},
            ],
            temperature=temperature,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            response_format={"type": "json_object"},
        )
        if seed is not None:
            kwargs["seed"] = seed

        resp = client.chat.completions.create(**kwargs)
        content = resp.choices[0].message.content or "{}"
        try:
            parsed = json.loads(content)
        except Exception:
            # –§–æ–ª–±—ç–∫ –Ω–∞ –º—è–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä
            m = re.search(r"\{.*\}", content, flags=re.DOTALL)
            if not m:
                raise RuntimeError("–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ-JSON –≤ –±–∞—Ç—á–µ %d" % i)
            parsed = json.loads(m.group(0))

        # sanity-check: –≤—Å–µ –∫–ª—é—á–∏ –Ω–∞ –º–µ—Å—Ç–µ
        for k in batch_keys:
            if k not in parsed:
                parsed[k] = sub_map[k]
        out.update({k: str(parsed[k]) for k in batch_keys})

    return out


def _call_openai_text(
    client: OpenAI,
    model: str,
    system_prompt: str,
    user_text: str,
    temperature: float,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
    seed: int | None = None,
) -> str:
    kwargs = dict(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text},
        ],
        temperature=temperature,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
    )
    if seed is not None:
        kwargs["seed"] = seed
    resp = client.chat.completions.create(**kwargs)
    return (resp.choices[0].message.content or "").strip()


# ----------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
# ----------------------------

def read_text_file(uploaded) -> str:
    raw = uploaded.read().decode("utf-8", errors="ignore")
    return raw


def read_docx_file(uploaded) -> str:
    if Document is None:
        raise RuntimeError("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–∞–∫–µ—Ç python-docx. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
    uploaded.seek(0)
    doc = Document(uploaded)
    blocks: List[str] = []
    for p in doc.paragraphs:
        if p.text is not None:
            blocks.append(p.text)
    for table in doc.tables:
        for row in table.rows:
            blocks.append("\t".join(cell.text for cell in row.cells))
    return "\n".join(block for block in blocks if block is not None)


def read_doc_file(uploaded) -> str:
    if textract is None:
        raise RuntimeError("–î–ª—è —á—Ç–µ–Ω–∏—è .doc —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ textract: pip install textract")
    uploaded.seek(0)
    with tempfile.NamedTemporaryFile(suffix=".doc", delete=True) as tmp:
        tmp.write(uploaded.read())
        tmp.flush()
        data = textract.process(tmp.name)
    return data.decode("utf-8", errors="ignore")


# ----------------------------
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
# ----------------------------

def build_docx_bytes(plain_text: str) -> bytes:
    if Document is None:
        raise RuntimeError("–î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ .docx —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ python-docx: pip install python-docx")
    doc = Document()
    for para in plain_text.split("\n"):
        doc.add_paragraph(para)
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()


# ----------------------------
# UI
# ----------------------------
col_in, col_opts = st.columns([2, 1], gap="large")

with col_in:
    st.markdown("#### –í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ HTML")
    input_text = st.text_area(
        "", height=280,
        placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –≤–∞—à —Ç–µ–∫—Å—Ç / HTML. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.",
    )
    uploaded = st.file_uploader(
        "‚Ä¶–∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (.html, .txt, .md, .docx, .doc)",
        type=["html", "txt", "md", "docx", "doc"],
        accept_multiple_files=False,
    )
    if uploaded is not None and not input_text:
        ext = (uploaded.name.split(".")[-1] or "").lower()
        try:
            if ext in {"html", "htm"}:
                input_text = read_text_file(uploaded)
            elif ext in {"txt", "md"}:
                input_text = read_text_file(uploaded)
            elif ext == "docx":
                input_text = read_docx_file(uploaded)
            elif ext == "doc":
                input_text = read_doc_file(uploaded)
            else:
                st.error("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
        except Exception as e:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")

with col_opts:
    st.markdown("#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
    api_key = st.text_input("OPENAI_API_KEY", value=API_KEY_DEFAULT, type="password")
    model_id = st.text_input("–ú–æ–¥–µ–ª—å", value=MODEL_DEFAULT, help="–ù–∞–ø—Ä.: gpt-4o, gpt-4o-mini, gpt-5")

    temperature = st.slider("temperature", 0.0, 1.5, 0.7, 0.1)
    top_p        = st.slider("top_p",        0.1, 1.0, 1.0, 0.05)
    freq_pen     = st.slider("frequency_penalty", -2.0, 2.0, 0.2, 0.1)
    pres_pen     = st.slider("presence_penalty",  -2.0, 2.0, 0.0, 0.1)
    seed_opt     = st.text_input("seed (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", value="", help="–î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏. –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏.")
    seed = int(seed_opt) if seed_opt.strip().isdigit() else None

    st.markdown("#### –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç")
    out_format = st.radio("–§–æ—Ä–º–∞—Ç –≤—ã–¥–∞—á–∏", ["HTML", "Plain/Markdown"], index=0, horizontal=True)
    add_words_marker = st.checkbox("–î–æ–±–∞–≤–ª—è—Ç—å [Words: N] –≤ –∫–æ–Ω–µ—Ü", value=False)
    wc_tol = st.slider("–î–æ–ø—É—Å—Ç–∏–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω—ã (¬±%)", 1, 20, 8)

    text_download_fmt = st.selectbox("–°–∫–∞—á–∞—Ç—å —Ç–µ–∫—Å—Ç –∫–∞–∫", ["TXT", "MD", "DOCX"], index=0, help="–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, –∫–æ–≥–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —Ç–µ–∫—Å—Ç.")

    st.markdown("#### –û–±—Ä–∞–±–æ—Ç–∞—Ç—å")
    go = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", type="primary", use_container_width=True)


# ----------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ----------------------------
if go:
    if not (input_text and input_text.strip()):
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
    elif not api_key:
        st.error(
            "–ù–µ –Ω–∞–π–¥–µ–Ω OPENAI_API_KEY. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –≤ Streamlit secrets (Settings ‚Üí Secrets) –∏–ª–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è."
        )
    else:
        try:
            client = _openai_client(api_key)
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å—é‚Ä¶"):
                if is_html(input_text):
                    # HTML ‚Üí JSON-–∑–∞–º–µ–Ω–∞ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–≥–∏ 1:1)
                    html_with_ids, mapping = extract_text_nodes_as_mapping(input_text)
                    rewritten_map = _call_openai_json_map(
                        client=client,
                        model=model_id,
                        mapping=mapping,
                        wc_tol=wc_tol,
                        temperature=temperature,
                        top_p=top_p,
                        frequency_penalty=freq_pen,
                        presence_penalty=pres_pen,
                        seed=seed,
                    )
                    result_html = replace_text_nodes_from_mapping(html_with_ids, rewritten_map)

                    # –°—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞ –ø–æ –≤–∏–¥–∏–º–æ–º—É —Ç–µ–∫—Å—Ç—É
                    if add_words_marker:
                        visible_text = BeautifulSoup(result_html, "lxml").get_text(separator=" ").strip()
                        words_n = _word_count(visible_text)
                        result_html = append_words_marker_to_html(result_html, words_n)

                    if out_format == "HTML":
                        result = result_html
                        out_kind = "html"
                    else:
                        plain = BeautifulSoup(result_html, "lxml").get_text(separator="\n")
                        plain = re.sub(r"\n{3,}", "\n\n", plain).strip()
                        if add_words_marker:
                            # –î–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–æ–±–∞–≤–∏–º –º–∞—Ä–∫–µ—Ä –≤ –∫–æ–Ω—Ü–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π
                            words_n = _word_count(plain)
                            plain = f"{plain}\n[Words: {words_n}]"
                        result = plain
                        out_kind = "txt"
                else:
                    # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç/Markdown
                    if out_format == "HTML":
                        sys_prompt = PROMPT_PLAIN_TO_HTML_TPL.format(wc_tol=wc_tol)
                        result = _call_openai_text(
                            client, model_id, sys_prompt, input_text,
                            temperature, top_p, freq_pen, pres_pen, seed
                        )
                        out_kind = "html"
                    else:
                        sys_prompt = PROMPT_PLAIN_TEXT_TPL.format(wc_tol=wc_tol)
                        text_out = _call_openai_text(
                            client, model_id, sys_prompt, input_text,
                            temperature, top_p, freq_pen, pres_pen, seed
                        )
                        if not add_words_marker:
                            # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä –æ—Ç–∫–ª—é—á—ë–Ω ‚Äî —É–¥–∞–ª–∏–º –µ–≥–æ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—Å—ë –∂–µ –¥–æ–±–∞–≤–∏–ª–∞
                            text_out = re.sub(r"\s*\[Words:\s*\d+\]\s*$", "", text_out).rstrip()
                        result = text_out
                        out_kind = "txt"

            # –í—ã–≤–æ–¥ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            st.success("–ì–æ—Ç–æ–≤–æ!")
            if out_kind == "html":
                st.markdown("–ü—Ä–æ—Å–º–æ—Ç—Ä HTML:")
                st.components.v1.html(result, height=600, scrolling=True)
                st.download_button(
                    label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .html",
                    data=result.encode("utf-8"),
                    file_name="humanized.html",
                    mime="text/html",
                )
                with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å HTML-–∫–æ–¥"):
                    st.code(result, language="html")
            else:
                st.markdown("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ç–µ–∫—Å—Ç–∞:")
                st.text_area("", value=result, height=400)

                fmt = text_download_fmt.upper()
                if fmt == "TXT":
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .txt",
                        data=result.encode("utf-8"),
                        file_name="humanized.txt",
                        mime="text/plain",
                    )
                elif fmt == "MD":
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .md",
                        data=result.encode("utf-8"),
                        file_name="humanized.md",
                        mime="text/markdown",
                    )
                elif fmt == "DOCX":
                    try:
                        docx_bytes = build_docx_bytes(result)
                        st.download_button(
                            label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .docx",
                            data=docx_bytes,
                            file_name="humanized.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        )
                    except Exception as e:
                        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å .docx: {e}")
        except Exception as e:
            # –ß–∏—Ç–∞–µ–º–∞—è –æ—à–∏–±–∫–∞ –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤/–º—É—Å–æ—Ä–∞
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
