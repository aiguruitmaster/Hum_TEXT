from __future__ import annotations

import io
import json
import re
import tempfile
from typing import Dict, Tuple

import streamlit as st
from bs4 import BeautifulSoup, NavigableString, Tag
from openai import OpenAI

# --- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ—Ñ–∏—Å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ---
try:
    from docx import Document  # .docx
except Exception:
    Document = None

try:
    import textract  # .doc (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –µ—Å—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
except Exception:
    textract = None

# ----------------------------
# –ö–ª—é—á –∏ –º–æ–¥–µ–ª—å –∏–∑ Streamlit Secrets / –æ–∫—Ä—É–∂–µ–Ω–∏—è
# ----------------------------
import os

API_KEY  = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY", "")
MODEL_ID = st.secrets.get("OPENAI_MODEL", os.getenv("OPENAI_MODEL", "gpt-5"))

# ----------------------------
# –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
# ----------------------------
st.set_page_config(page_title="Humanizer ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã", page_icon="üõ†Ô∏è", layout="wide")
st.title("üõ†Ô∏è Humanizer —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")

# ----------------------------
# –•–µ–ª–ø–µ—Ä—ã
# ----------------------------
def is_html(text: str) -> bool:
    if not text:
        return False
    has_tag = bool(re.search(r"<([a-zA-Z][^>]*?)>", text))
    has_angle = "</" in text or "/>" in text
    return has_tag and has_angle

def extract_text_nodes_as_mapping(html: str) -> Tuple[str, Dict[str, str]]:
    """–û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã –≤ <span data-hid="..."> –∏ —Å–æ–±–∏—Ä–∞–µ—Ç mapping id->text."""
    soup = BeautifulSoup(html, "lxml")
    for bad in soup(["script", "style", "noscript"]):
        bad.extract()

    hid_counter = 0
    mapping: Dict[str, str] = {}

    def tag_text_nodes(t: Tag) -> None:
        nonlocal hid_counter
        for child in list(t.children):
            if isinstance(child, NavigableString):
                text = str(child)
                if text and text.strip():
                    hid_counter += 1
                    hid = f"t{hid_counter}"
                    span = soup.new_tag("span")
                    span["data-hid"] = hid
                    span.string = text
                    child.replace_with(span)
                    mapping[hid] = text
            elif isinstance(child, Tag):
                tag_text_nodes(child)

    tag_text_nodes(soup.body or soup)
    return str(soup), mapping

def replace_text_nodes_from_mapping(html_with_ids: str, replacements: Dict[str, str]) -> str:
    soup = BeautifulSoup(html_with_ids, "lxml")
    for span in soup.find_all(attrs={"data-hid": True}):
        hid = span.get("data-hid")
        if hid in replacements:
            span.string = replacements[hid]
    for span in soup.find_all(attrs={"data-hid": True}):
        del span["data-hid"]
    return str(soup)

def _safe_json_loads(maybe_json: str) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏—Ç JSON-–æ–±—ä–µ–∫—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏. –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π {...} –±–ª–æ–∫."""
    try:
        return json.loads(maybe_json)
    except Exception:
        pass
    m = re.search(r"\{.*\}", maybe_json, flags=re.DOTALL)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass
    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.")

def _word_count(s: str) -> int:
    tokens = re.findall(r"\w+", s, flags=re.UNICODE)
    return len(tokens)

def append_words_marker_to_html(html: str, n: int) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–æ–Ω–µ—Ü HTML –≤–∏–¥–∏–º—ã–π –º–∞—Ä–∫–µ—Ä [Words: N] –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π <p>."""
    try:
        soup = BeautifulSoup(html, "lxml")
        container = soup.body or soup
        p = soup.new_tag("p")
        p.string = f"[Words: {n}]"
        container.append(p)
        return str(soup)
    except Exception:
        return f"{html}\n[Words: {n}]"

# ----------------------------
# –ü—Ä–æ–º–ø—Ç—ã (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ)
# ----------------------------

# 1) –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –û–¢–†–ï–î–ê–ö–¢–ò–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢ —Å [Words: N] –≤ –∫–æ–Ω—Ü–µ
PROMPT_PLAIN_TEXT = """You are an expert human editor.

Goal
- Make the text read like it was written by a human native speaker.
- Keep meaning, facts, entities, URLs, numbers, dates, titles, and overall structure.

Language
- Use the SAME language as the input (auto-detect). Do NOT translate or normalize dialect/orthography.

Constraints
- Word count: keep within ¬±10% of the original. Append the final count as [Words: N].
- Preserve paragraph breaks, headings, list order/numbering, and section order.
- Preserve punctuation, quotation marks, inline formatting markers (bold/italic/links/code), emojis, citation markers, and references.

Style targets (human-like)
- Vary sentence length and rhythm; mix short and long sentences (‚Äúburstiness‚Äù).
- Prefer specific, idiomatic phrasing over generic templates; avoid stock openings like ‚ÄúIn conclusion,‚Äù ‚ÄúAs we can see,‚Äù etc.
- Use natural connectors (however, meanwhile, notably, still, that said, in fact, at times, for instance) but not in a repetitive pattern.
- Keep the author‚Äôs voice and register; do not add opinions or new facts.

Do NOT
- Do not add or remove factual content.
- Do not change any code blocks, formulas, or tables.

Output
- Return ONLY the edited text (no explanations, no code fences), with [Words: N] at the end.
"""

# 2) –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –∫—Ä–∞—Å–∏–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π HTML –ò —Ç–æ–∂–µ –¥–æ–±–∞–≤–∏—Ç—å [Words: N] –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π <p>.
PROMPT_PLAIN_TO_HTML = """You are an expert human editor and HTML formatter.

Goal
- Make the text read like a human native speaker wrote it.
- Then output clean, semantic HTML for the edited content.

Language
- Use the SAME language as the input (auto-detect). Do NOT translate or normalize dialect.

Constraints
- Word count: keep within ¬±10% of the original. Append the final count as the LAST paragraph: [Words: N].
- Preserve paragraph breaks, headings, list order/numbering, and section order; convert to equivalent HTML.
- Preserve punctuation, quotation marks, emphasis/links/code semantics; convert inline markers to <strong>/<em>/<a>/<code>.
- Keep facts, names, numbers, dates, URLs, and titles intact.

Style targets (human-like)
- Vary sentence length and rhythm; avoid template phrasing and repetitive transitions.
- Keep the author‚Äôs voice and register; improve fluency without changing intent.

Tables
- If there is at least one <table> in the edited content, include at the VERY TOP exactly one style block:
  <style>
  table { border-collapse: collapse; }
  table, th, td { border: 1px solid #000; }
  </style>
  Do not include any other CSS or inline styles.

Non-text
- Leave code blocks, formulas, tables as-is but wrap appropriately (<pre><code>, <table>, etc.) if present.

Allowed tags
- style (single block as above), p, h1..h4, ul/ol/li, blockquote, pre, code, a, strong, em,
  table/thead/tbody/tr/th/td, img (only if present in input), span.

Output
- Return ONLY the HTML markup. No markdown, no comments, no code fences, no explanations.
"""

# 3) HTML —á–µ—Ä–µ–∑ JSON-–∑–∞–º–µ–Ω—É (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É 1:1); [Words: N] –¥–æ–±–∞–≤–∏–º –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.
PROMPT_HTML_JSON = """You are an expert micro-editor.

Input
- You will receive a JSON object mapping {id: text}, extracted from HTML text nodes.
- Edit each VALUE so it reads like natural human writing while keeping meaning and tone.

Language
- Use the SAME language as each value (auto-detect). Do NOT translate or normalize dialect.

Per-value constraints
- Word count: keep within ¬±10% of that value‚Äôs original length.
- Preserve punctuation, quotation marks, inline formatting markers present in the value,
  emojis, citation markers, and references.
- Absolutely DO NOT introduce or remove HTML tags (you edit TEXT ONLY).
- Keep facts, names, numbers, dates, URLs, and titles unchanged.

Style targets (human-like)
- Vary rhythm (short/long sentences where applicable); avoid generic templates and clich√©s.
- Maintain voice and register; do not add opinions or new information.

Output format (strict)
- Return ONLY a VALID JSON OBJECT with the SAME KEYS and improved string values.
- No surrounding text, no code fences, no comments.
- If any value is empty or whitespace, copy it unchanged.

Begin by returning the JSON object for the provided mapping.
"""

# ----------------------------
# –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—è–º–∏
# ----------------------------
def call_openai_json_map(api_key: str, mapping: Dict[str, str]) -> Dict[str, str]:
    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=MODEL_ID,
        messages=[
            {"role": "user", "content": PROMPT_HTML_JSON},
            {"role": "user", "content": json.dumps(mapping, ensure_ascii=False)},
        ],
    )
    content = resp.choices[0].message.content or "{}"
    return _safe_json_loads(content)

def call_openai_rewrite_text(api_key: str, text: str) -> str:
    """–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (+[Words: N])."""
    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=MODEL_ID,
        messages=[
            {"role": "user", "content": PROMPT_PLAIN_TEXT},
            {"role": "user", "content": text},
        ],
    )
    return (resp.choices[0].message.content or "").strip()

def call_openai_rewrite_text_to_html(api_key: str, text: str) -> str:
    """–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –∫—Ä–∞—Å–∏–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π HTML —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º <p>[Words: N]</p>."""
    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=MODEL_ID,
        messages=[
            {"role": "user", "content": PROMPT_PLAIN_TO_HTML},
            {"role": "user", "content": text},
        ],
    )
    return (resp.choices[0].message.content or "").strip()

# ----------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
# ----------------------------
def read_text_file(uploaded) -> str:
    raw = uploaded.read().decode("utf-8", errors="ignore")
    return raw

def read_docx_file(uploaded) -> str:
    if Document is None:
        raise RuntimeError("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–∞–∫–µ—Ç python-docx. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
    uploaded.seek(0)
    doc = Document(uploaded)
    blocks = []
    for p in doc.paragraphs:
        blocks.append(p.text)
    for table in doc.tables:
        for row in table.rows:
            blocks.append("\t".join(cell.text for cell in row.cells))
    return "\n".join(block for block in blocks if block is not None)

def read_doc_file(uploaded) -> str:
    if textract is None:
        raise RuntimeError("–î–ª—è —á—Ç–µ–Ω–∏—è .doc —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ textract: pip install textract")
    uploaded.seek(0)
    with tempfile.NamedTemporaryFile(suffix=".doc", delete=True) as tmp:
        tmp.write(uploaded.read())
        tmp.flush()
        data = textract.process(tmp.name)
    return data.decode("utf-8", errors="ignore")

# ----------------------------
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
# ----------------------------
def build_docx_bytes(plain_text: str) -> bytes:
    if Document is None:
        raise RuntimeError("–î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ .docx —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ python-docx: pip install python-docx")
    doc = Document()
    for para in plain_text.split("\n"):
        doc.add_paragraph(para)
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

# ----------------------------
# UI: –≤–≤–æ–¥ –∏ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç
# ----------------------------
col_in, col_opts = st.columns([2, 1], gap="large")

with col_in:
    st.markdown("#### –í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ HTML")
    input_text = st.text_area(
        "", height=280,
        placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –≤–∞—à —Ç–µ–∫—Å—Ç / HTML. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.",
    )
    uploaded = st.file_uploader(
        "‚Ä¶–∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (.html, .txt, .md, .docx, .doc)",
        type=["html", "txt", "md", "docx", "doc"],
        accept_multiple_files=False
    )
    if uploaded is not None and not input_text:
        ext = (uploaded.name.split(".")[-1] or "").lower()
        try:
            if ext in {"html", "htm"}:
                input_text = read_text_file(uploaded)
            elif ext in {"txt", "md"}:
                input_text = read_text_file(uploaded)
            elif ext == "docx":
                input_text = read_docx_file(uploaded)
            elif ext == "doc":
                input_text = read_doc_file(uploaded)
            else:
                st.error("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
        except Exception as e:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")

with col_opts:
    st.markdown("#### –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç")
    out_format = st.radio("–§–æ—Ä–º–∞—Ç –≤—ã–¥–∞—á–∏", ["HTML", "Plain/Markdown"], index=0, horizontal=True)
    text_download_fmt = st.selectbox("–°–∫–∞—á–∞—Ç—å —Ç–µ–∫—Å—Ç –∫–∞–∫", ["TXT", "MD", "DOCX"], index=0, help="–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, –∫–æ–≥–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —Ç–µ–∫—Å—Ç.")
    st.markdown("#### –û–±—Ä–∞–±–æ—Ç–∞—Ç—å")
    go = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ö—É–º–∞–Ω–∏–∑–∞—Ü–∏—é", type="primary", use_container_width=True)

# ----------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ----------------------------
if go:
    if not input_text or not input_text.strip():
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
    elif not API_KEY:
        st.error(
            "–ù–µ –Ω–∞–π–¥–µ–Ω OPENAI_API_KEY. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –≤ Streamlit secrets "
            "(Settings ‚Üí Secrets) –∏–ª–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è."
        )
    else:
        try:
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å—é‚Ä¶"):
                if is_html(input_text):
                    # HTML ‚Üí JSON-–∑–∞–º–µ–Ω–∞ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–≥–∏ 1:1). [Words: N] –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ —Å–±–æ—Ä–∫–∏.
                    html_with_ids, mapping = extract_text_nodes_as_mapping(input_text)
                    rewritten_map = call_openai_json_map(API_KEY, mapping)
                    result_html = replace_text_nodes_from_mapping(html_with_ids, rewritten_map)

                    # –°—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞ –ø–æ –≤–∏–¥–∏–º–æ–º—É —Ç–µ–∫—Å—Ç—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä
                    visible_text = BeautifulSoup(result_html, "lxml").get_text(separator=" ").strip()
                    words_n = _word_count(visible_text)
                    result_html = append_words_marker_to_html(result_html, words_n)

                    if out_format == "HTML":
                        result = result_html
                        out_kind = "html"
                    else:
                        plain = BeautifulSoup(result_html, "lxml").get_text(separator="\n")
                        plain = re.sub(r"\n{3,}", "\n\n", plain).strip()
                        result = plain
                        out_kind = "txt"
                else:
                    # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç/Markdown
                    if out_format == "HTML":
                        # –ö—Ä–∞—Å–∏–≤—ã–π HTML ‚Äî –ø—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏—Ç—å [Words: N] –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ.
                        result = call_openai_rewrite_text_to_html(API_KEY, input_text)
                        out_kind = "html"
                    else:
                        # –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å [Words: N] –≤ –∫–æ–Ω—Ü–µ (–¥–æ–±–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å)
                        result = call_openai_rewrite_text(API_KEY, input_text)
                        out_kind = "txt"

            # –í—ã–≤–æ–¥ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            st.success("–ì–æ—Ç–æ–≤–æ!")
            if out_kind == "html":
                st.markdown("–ü—Ä–æ—Å–º–æ—Ç—Ä HTML:")
                st.components.v1.html(result, height=600, scrolling=True)
                st.download_button(
                    label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .html",
                    data=result.encode("utf-8"),
                    file_name="humanized.html",
                    mime="text/html",
                )
                with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å HTML-–∫–æ–¥"):
                    st.code(result, language="html")
            else:
                st.markdown("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ç–µ–∫—Å—Ç–∞:")
                st.text_area("", value=result, height=400)

                fmt = text_download_fmt.upper()
                if fmt == "TXT":
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .txt",
                        data=result.encode("utf-8"),
                        file_name="humanized.txt",
                        mime="text/plain",
                    )
                elif fmt == "MD":
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .md",
                        data=result.encode("utf-8"),
                        file_name="humanized.md",
                        mime="text/markdown",
                    )
                elif fmt == "DOCX":
                    try:
                        docx_bytes = build_docx_bytes(result)
                        st.download_button(
                            label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .docx",
                            data=docx_bytes,
                            file_name="humanized.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        )
                    except Exception as e:
                        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å .docx: {e}")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
