from __future__ import annotations
import io
import json
import re
import tempfile
from typing import Dict, Tuple
import streamlit as st
from bs4 import BeautifulSoup, NavigableString, Tag
from anthropic import Anthropic
# --- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ—Ñ–∏—Å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ---
try:
    from docx import Document # .docx
except Exception:
    Document = None
try:
    import textract # .doc (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –µ—Å—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
except Exception:
    textract = None
# ----------------------------
# –ö–ª—é—á –∏ –º–æ–¥–µ–ª—å –∏–∑ Streamlit Secrets / –æ–∫—Ä—É–∂–µ–Ω–∏—è
# ----------------------------
import os
API_KEY = st.secrets.get("ANTHROPIC_API_KEY") or os.getenv("ANTHROPIC_API_KEY", "")
MODEL_ID = st.secrets.get("ANTHROPIC_MODEL", os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20240620"))
# ----------------------------
# –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
# ----------------------------
st.set_page_config(page_title="Humanizer ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã", page_icon="üõ†Ô∏è", layout="wide")
st.title("üõ†Ô∏è Humanizer —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
# ----------------------------
# –•–µ–ª–ø–µ—Ä—ã
# ----------------------------
def is_html(text: str) -> bool:
    if not text:
        return False
    has_tag = bool(re.search(r"<([a-zA-Z][^>]*?)>", text))
    has_angle = "</" in text or "/>" in text
    return has_tag and has_angle
def extract_text_nodes_as_mapping(html: str) -> Tuple[str, Dict[str, str]]:
    """–û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã –≤ <span data-hid="..."> –∏ —Å–æ–±–∏—Ä–∞–µ—Ç mapping id->text."""
    soup = BeautifulSoup(html, "lxml")
    for bad in soup(["script", "style", "noscript"]):
        bad.extract()
    hid_counter = 0
    mapping: Dict[str, str] = {}
    def tag_text_nodes(t: Tag) -> None:
        nonlocal hid_counter
        for child in list(t.children):
            if isinstance(child, NavigableString):
                text = str(child)
                if text and text.strip():
                    hid_counter += 1
                    hid = f"t{hid_counter}"
                    span = soup.new_tag("span")
                    span["data-hid"] = hid
                    span.string = text
                    child.replace_with(span)
                    mapping[hid] = text
            elif isinstance(child, Tag):
                tag_text_nodes(child)
    tag_text_nodes(soup.body or soup)
    return str(soup), mapping
def replace_text_nodes_from_mapping(html_with_ids: str, replacements: Dict[str, str]) -> str:
    soup = BeautifulSoup(html_with_ids, "lxml")
    for span in soup.find_all(attrs={"data-hid": True}):
        hid = span.get("data-hid")
        if hid in replacements:
            span.string = replacements[hid]
    for span in soup.find_all(attrs={"data-hid": True}):
        del span["data-hid"]
    return str(soup)
def _safe_json_loads(maybe_json: str) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏—Ç JSON-–æ–±—ä–µ–∫—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏. –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π {...} –±–ª–æ–∫."""
    try:
        return json.loads(maybe_json)
    except Exception:
        pass
    m = re.search(r"\{.*\}", maybe_json, flags=re.DOTALL)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass
    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.")
def _word_count(s: str) -> int:
    tokens = re.findall(r"\w+", s, flags=re.UNICODE)
    return len(tokens)
def append_words_marker_to_html(html: str, n: int) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–æ–Ω–µ—Ü HTML –≤–∏–¥–∏–º—ã–π –º–∞—Ä–∫–µ—Ä [Words: N] –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π <p>."""
    try:
        soup = BeautifulSoup(html, "lxml")
        container = soup.body or soup
        p = soup.new_tag("p")
        p.string = f"[Words: {n}]"
        container.append(p)
        return str(soup)
    except Exception:
        return f"{html}\n[Words: {n}]"
# ----------------------------
# –ü—Ä–æ–º–ø—Ç—ã
# ----------------------------
# 1) –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (+[Words: N]).
PROMPT_PLAIN_TEXT = """Task: Edit the text so it reads naturally and fluently for a native speaker while preserving the original meaning, structure, and tone.
Language: Use the SAME language as the input text (auto-detect). Do NOT translate. Preserve the original dialect/orthography (e.g., en-GB vs en-US).
Requirements:
- Word count: keep within ¬±2% of the original. Append the final count as [Words: N].
- Structure: keep the same paragraph breaks, headings, list order/numbering, and overall section order.
- Formatting: keep punctuation, quotation marks, inline formatting (bold/italic/links/code), emojis, citation markers, and references exactly as they are.
- Facts & entities: do not add, remove, or alter information. Keep names, numbers, dates, URLs, and titles unchanged.
- Tone & register: preserve the author‚Äôs voice, level of formality, and rhetorical stance.
- Style tweaks: replace awkward phrasing with idiomatic expressions, reduce repetitiveness, vary sentence length for burstiness (mix short and long sentences), and simplify clunky constructions‚Äîwithout changing emphasis or intent. Increase perplexity by using unexpected but natural phrasing, contractions (e.g., don't instead of do not), colloquialisms, and varied vocabulary to avoid predictable patterns.
- Non-text elements (code, formulas, tables): leave unchanged.
- If perfect word-count preservation would hurt clarity or grammar, prefer clarity but stay as close as possible to the target range.
- Return ONLY the edited text‚Äîno explanations, no metadata (besides [Words: N]), no code fences.
Input:
"""
# 2) –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –∫—Ä–∞—Å–∏–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π HTML –ò —Ç–æ–∂–µ –¥–æ–±–∞–≤–∏—Ç—å [Words: N] –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π <p>.
PROMPT_PLAIN_TO_HTML = """Task: Edit the text so it reads naturally and fluently for a native speaker while preserving the original meaning, structure, and tone ‚Äî then output clean, semantic HTML for the edited text.
Language: Use the SAME language as the input text (auto-detect). Do NOT translate. Preserve the original dialect/orthography.
Requirements:
- Word count: keep within ¬±2% of the original. Append the final count as a visible last paragraph: [Words: N].
- Keep the same paragraph breaks, headings, list order/numbering, and overall section order as in the input. Convert them to equivalent HTML structure.
- Preserve punctuation, quotation marks, inline emphasis/links/code semantics; convert inline formatting markers to their HTML equivalents (<strong>/<em>/<a>/<code>).
- Do not add, remove, or alter facts, names, numbers, dates, URLs, or titles.
- Preserve the author‚Äôs voice and register; improve fluency without changing intent. Vary sentence length for burstiness (mix short and long), increase perplexity with unexpected natural phrasing, use contractions, idioms, and varied vocab to mimic human writing and avoid AI detection patterns.
- If the edited content contains at least one <table>, INCLUDE at the very top of the output a SINGLE <style> block:
  <style>
  table { border-collapse: collapse; }
  table, th, td { border: 1px solid #000; }
  </style>
  Do not include any other CSS or inline style attributes.
- Non-text elements (code blocks, formulas, tables) should be kept as-is but wrapped in appropriate HTML tags (<pre><code>, <table>, etc.) if present.
- Return ONLY the HTML markup of the edited text. No markdown, no comments, no code fences, no explanations.
- Allowed tags include: style (single block as specified above), p, h1..h4, ul/ol/li, blockquote, pre, code, a, strong, em, table/thead/tbody/tr/th/td, img (only if present in input), span.
Input:
"""
# 3) HTML —á–µ—Ä–µ–∑ JSON-–∑–∞–º–µ–Ω—É (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É 1:1); [Words: N] –¥–æ–±–∞–≤–∏–º –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.
PROMPT_HTML_JSON = """You will be provided with a JSON object containing key-value pairs where keys are IDs and values are text strings extracted from HTML.

Task: Edit each value so it reads naturally and fluently for a native speaker while preserving the original meaning, structure, and tone.

Language: Use the SAME language as each input value (auto-detect). Do NOT translate. Preserve the original dialect/orthography.

Requirements (APPLY PER VALUE):
- Word count: keep within ¬±2% of that value‚Äôs original length. Do NOT add any extra markers like [Words: N].
- Keep punctuation, quotation marks, inline formatting, emojis, citation markers, and references exactly as they are in the value.
- Do not add, remove, or alter facts, names, numbers, dates, URLs, or titles.
- Preserve the author‚Äôs voice and register.
- Absolutely do NOT introduce or remove HTML tags; you are editing TEXT CONTENT ONLY.
- Improve fluency by varying sentence length for burstiness (mix short/long), increasing perplexity with unexpected natural phrasing, using contractions, idioms, and varied vocabulary to avoid predictable AI patterns.
- Return ONLY a valid JSON object with the SAME KEYS and improved string values. No comments, no code fences, no extra text, no explanations. Ensure the output is parseable as JSON.
"""
# ----------------------------
# –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—è–º–∏
# ----------------------------
def call_anthropic_json_map(api_key: str, mapping: Dict[str, str]) -> Dict[str, str]:
    client = Anthropic(api_key=api_key)
    input_json = json.dumps(mapping, ensure_ascii=False)
    resp = client.messages.create(
        model=MODEL_ID,
        max_tokens=8192,  # –£–≤–µ–ª–∏—á—å—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
        system=PROMPT_HTML_JSON,
        messages=[
            {"role": "user", "content": input_json},
        ],
    )
    content = resp.content[0].text if resp.content else "{}"
    return _safe_json_loads(content)
def call_anthropic_rewrite_text(api_key: str, text: str) -> str:
    """–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (+[Words: N])."""
    client = Anthropic(api_key=api_key)
    resp = client.messages.create(
        model=MODEL_ID,
        max_tokens=8192,
        messages=[
            {"role": "user", "content": PROMPT_PLAIN_TEXT + text},
        ],
    )
    return (resp.content[0].text or "").strip()
def call_anthropic_rewrite_text_to_html(api_key: str, text: str) -> str:
    """–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –∫—Ä–∞—Å–∏–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π HTML —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º <p>[Words: N]</p>."""
    client = Anthropic(api_key=api_key)
    resp = client.messages.create(
        model=MODEL_ID,
        max_tokens=8192,
        messages=[
            {"role": "user", "content": PROMPT_PLAIN_TO_HTML + text},
        ],
    )
    return (resp.content[0].text or "").strip()
# ----------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
# ----------------------------
def read_text_file(uploaded) -> str:
    raw = uploaded.read().decode("utf-8", errors="ignore")
    return raw
def read_docx_file(uploaded) -> str:
    if Document is None:
        raise RuntimeError("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–∞–∫–µ—Ç python-docx. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
    uploaded.seek(0)
    doc = Document(uploaded)
    blocks = []
    for p in doc.paragraphs:
        blocks.append(p.text)
    for table in doc.tables:
        for row in table.rows:
            blocks.append("\t".join(cell.text for cell in row.cells))
    return "\n".join(block for block in blocks if block is not None)
def read_doc_file(uploaded) -> str:
    if textract is None:
        raise RuntimeError("–î–ª—è —á—Ç–µ–Ω–∏—è .doc —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ textract: pip install textract")
    uploaded.seek(0)
    with tempfile.NamedTemporaryFile(suffix=".doc", delete=True) as tmp:
        tmp.write(uploaded.read())
        tmp.flush()
        data = textract.process(tmp.name)
    return data.decode("utf-8", errors="ignore")
# ----------------------------
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
# ----------------------------
def build_docx_bytes(plain_text: str) -> bytes:
    if Document is None:
        raise RuntimeError("–î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ .docx —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ python-docx: pip install python-docx")
    doc = Document()
    for para in plain_text.split("\n"):
        doc.add_paragraph(para)
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()
# ----------------------------
# UI: –≤–≤–æ–¥ –∏ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç
# ----------------------------
col_in, col_opts = st.columns([2, 1], gap="large")
with col_in:
    st.markdown("#### –í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ HTML")
    input_text = st.text_area(
        "", height=280,
        placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –≤–∞—à —Ç–µ–∫—Å—Ç / HTML. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.",
    )
    uploaded = st.file_uploader(
        "‚Ä¶–∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (.html, .txt, .md, .docx, .doc)",
        type=["html", "txt", "md", "docx", "doc"],
        accept_multiple_files=False
    )
    if uploaded is not None and not input_text:
        ext = (uploaded.name.split(".")[-1] or "").lower()
        try:
            if ext in {"html", "htm"}:
                input_text = read_text_file(uploaded)
            elif ext in {"txt", "md"}:
                input_text = read_text_file(uploaded)
            elif ext == "docx":
                input_text = read_docx_file(uploaded)
            elif ext == "doc":
                input_text = read_doc_file(uploaded)
            else:
                st.error("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
        except Exception as e:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
with col_opts:
    st.markdown("#### –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç")
    out_format = st.radio("–§–æ—Ä–º–∞—Ç –≤—ã–¥–∞—á–∏", ["HTML", "Plain/Markdown"], index=0, horizontal=True)
    text_download_fmt = st.selectbox("–°–∫–∞—á–∞—Ç—å —Ç–µ–∫—Å—Ç –∫–∞–∫", ["TXT", "MD", "DOCX"], index=0, help="–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, –∫–æ–≥–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —Ç–µ–∫—Å—Ç.")
    st.markdown("#### –û–±—Ä–∞–±–æ—Ç–∞—Ç—å")
    go = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ö—É–º–∞–Ω–∏–∑–∞—Ü–∏—é", type="primary", use_container_width=True)
# ----------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ----------------------------
if go:
    if not input_text or not input_text.strip():
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
    elif not API_KEY:
        st.error(
            "–ù–µ –Ω–∞–π–¥–µ–Ω ANTHROPIC_API_KEY. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –≤ Streamlit secrets "
            "(Settings ‚Üí Secrets) –∏–ª–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è."
        )
    else:
        try:
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å—é‚Ä¶"):
                if is_html(input_text):
                    # HTML ‚Üí JSON-–∑–∞–º–µ–Ω–∞ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–≥–∏ 1:1). [Words: N] –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ —Å–±–æ—Ä–∫–∏.
                    html_with_ids, mapping = extract_text_nodes_as_mapping(input_text)
                    rewritten_map = call_anthropic_json_map(API_KEY, mapping)
                    result_html = replace_text_nodes_from_mapping(html_with_ids, rewritten_map)
                    # –°—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞ –ø–æ –≤–∏–¥–∏–º–æ–º—É —Ç–µ–∫—Å—Ç—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä
                    visible_text = BeautifulSoup(result_html, "lxml").get_text(separator=" ").strip()
                    words_n = _word_count(visible_text)
                    result_html = append_words_marker_to_html(result_html, words_n)
                    if out_format == "HTML":
                        result = result_html
                        out_kind = "html"
                    else:
                        plain = BeautifulSoup(result_html, "lxml").get_text(separator="\n")
                        plain = re.sub(r"\n{3,}", "\n\n", plain).strip()
                        result = plain
                        out_kind = "txt"
                else:
                    # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç/Markdown
                    if out_format == "HTML":
                        # –ö—Ä–∞—Å–∏–≤—ã–π HTML ‚Äî –ø—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏—Ç—å [Words: N] –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ.
                        result = call_anthropic_rewrite_text_to_html(API_KEY, input_text)
                        out_kind = "html"
                    else:
                        # –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å [Words: N] –≤ –∫–æ–Ω—Ü–µ (–¥–æ–±–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å)
                        result = call_anthropic_rewrite_text(API_KEY, input_text)
                        out_kind = "txt"
            # –í—ã–≤–æ–¥ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            st.success("–ì–æ—Ç–æ–≤–æ!")
            if out_kind == "html":
                st.markdown("–ü—Ä–æ—Å–º–æ—Ç—Ä HTML:")
                st.components.v1.html(result, height=600, scrolling=True)
                st.download_button(
                    label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .html",
                    data=result.encode("utf-8"),
                    file_name="humanized.html",
                    mime="text/html",
                )
                with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å HTML-–∫–æ–¥"):
                    st.code(result, language="html")
            else:
                st.markdown("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ç–µ–∫—Å—Ç–∞:")
                st.text_area("", value=result, height=400)
                fmt = text_download_fmt.upper()
                if fmt == "TXT":
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .txt",
                        data=result.encode("utf-8"),
                        file_name="humanized.txt",
                        mime="text/plain",
                    )
                elif fmt == "MD":
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .md",
                        data=result.encode("utf-8"),
                        file_name="humanized.md",
                        mime="text/markdown",
                    )
                elif fmt == "DOCX":
                    try:
                        docx_bytes = build_docx_bytes(result)
                        st.download_button(
                            label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .docx",
                            data=docx_bytes,
                            file_name="humanized.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        )
                    except Exception as e:
                        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å .docx: {e}")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
